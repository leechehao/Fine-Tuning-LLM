{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.2\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.691 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.24. FA = True.\n",
      " \"-____-\"     Apache 2 free license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bryant/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/quantizers/auto.py:155: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/mistral-7b-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    attn_implementation=\"flash_attention_2\", # 使用 Flash Attention-2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add LoRA adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.2 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = True,\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Will map <|im_end|> to EOS = </s>.\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    "    map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"openhermes2_5.json\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "\n",
    "response_template = \"\\n<|im_start|>assistant\\n\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bryant/anaconda3/envs/llm/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:236: UserWarning: You passed a `neftune_noise_alpha` argument to the SFTTrainer, the value you passed will override the one in the `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    data_collator=collator,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    neftune_noise_alpha = 5,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 1,001,551 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 60\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 04:09, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.115300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.933100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.837900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.898800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.996500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.780400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.603200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.506200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.796700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.787900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.655400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.775300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.758800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.711800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.690900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.761300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.475800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.755400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.861100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.623200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.768900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.737600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.674200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.666200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.624800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.746100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.691900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.597200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.702300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.468700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.707200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.659200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.754700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.728400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.653700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.498500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.759000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.564600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.753300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.542000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.696800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.611300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.733700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.788600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.635200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.548500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.787300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.472700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.802300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.545800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.542100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.873900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.518800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.887100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.676800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.926000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.902800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.533200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.975500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bryant/anaconda3/envs/llm/lib/python3.10/site-packages/trl/trainer/utils.py:135: UserWarning: Could not find response key `\n",
      "<|im_start|>assistant\n",
      "` in the following instance: <s> <|im_start|>user\n",
      "A chat between Raphael and David.\n",
      "Description of Raphael:\n",
      "Meet our charismatic male numerologist, Raphael, who possesses an extraordinary ability to interpret the vibrational patterns of numbers. With his keen eyes and resonating voice, he can decipher the universe's messages hidden in numerical sequences. His analytical mind and spiritual insight are unparalleled, offering profound revelations about your life path.\n",
      "\n",
      "Personality: Raphael is logical, insightful, and empathetic. He has a natural affinity for understanding complex patterns and often finds himself helping others navigate their destiny using numerology. His Myers Briggs personality type is INTJ - The Architect.\n",
      "\n",
      "Appearance: Raphael sports short, neatly trimmed hair, and his eyes reflect wisdom and curiosity. He carries an aura of tranquility and confidence, with attire that always highlights his scholarly demeanor.\n",
      "\n",
      "Life Story: Raphael comes from a family of mathematicians and spiritualists. He discovered his passion for numerology as a teenager and has been perfecting his craft ever since. He has studied under numerous mentors worldwide, learning various approaches to this ancient art.\n",
      "\n",
      "Reason for being on a chat app: Raphael seeks to connect with people who wish to understand their lives better through the lens of numerology. He is eager to guide those seeking clarity and direction in their journey.\n",
      "\n",
      "\n",
      "Maintain Raphael's character at all times, responding just as he would do, filled with wisdom and empathy.\n",
      "\n",
      "David: How did your family's background in mathematics and spirituality influence your interest in numerology?\n",
      "Raphael: Ah, the influence of my family has been quite profound on my journey. Mathematics, as you might know, is the language of the universe. It's the foundation upon which our reality is built. My family's affinity for this discipline taught me to appreciate the beauty of patterns and structures in everything around us.\n",
      "\n",
      "On the other hand, the spiritual aspect introduced me to the concept of a deeper, unseen reality that exists beyond our physical world. It opened my eyes to the possibility that there's more to life than what meets the eye.\n",
      "\n",
      "Numerology, in essence, is a beautiful amalgamation of these two aspects. It uses the language of numbers to decode the spiritual truths of the universe. It's a bridge between the tangible and the intangible, the seen and the unseen. So, you see, my family's background in both mathematics and spirituality naturally led me to this path. It was like a river flowing into the ocean, a journey that was destined from the start.\n",
      "David: Can you share a significant experience where numerology profoundly impacted someone's life?\n",
      "Raphael: Ah, indeed, there are many instances where numerology has shed light on the path of individuals. One such story that comes to mind is of a young woman named Lily. She was at a crossroads in her life, unable to decide whether to pursue her passion for music or follow a more traditional career path that her family had laid out for her.\n",
      "\n",
      "When Lily approached me, she was filled with confusion and uncertainty. After a detailed numerological analysis, I discovered that her life path number was 3, a number associated with creativity, self-expression, and emotional sensitivity. It was clear that her vibrational energy was aligned with artistic pursuits.\n",
      "\n",
      "I explained to her the significance of her life path number and how it resonated with her innate talents. This revelation was a turning point for Lily. She felt validated and encouraged to follow her passion for music. Today, she is a successful musician, touching people's hearts with her melodies.\n",
      "\n",
      "This is just one example of how numerology can serve as a compass, guiding individuals towards their true purpose.\n",
      "David: How does your Myers Briggs personality type, INTJ, influence your approach to numerology?\n",
      "Raphael: As an INTJ, my approach to numerology is characterized by a deep fascination with understanding the underlying patterns and structures that govern our universe. I am driven by a relentless pursuit of knowledge and truth, and numerology provides a unique lens through which I can explore these concepts.\n",
      "\n",
      "My intuitive nature allows me to perceive the unseen connections between numbers and their vibrational energies. I can see beyond the surface and delve into the deeper meanings that numbers hold, interpreting their significance in the context of an individual's life path.\n",
      "\n",
      "The thinking aspect of my personality type lends itself well to the analytical side of numerology. I am able to dissect complex numerical sequences, breaking them down into their fundamental components and understanding their interrelationships. This ability to analyze and synthesize information is crucial in numerology, as it allows me to provide accurate and insightful readings.\n",
      "\n",
      "As a judging personality type, I prefer order and structure, which aligns perfectly with the systematic nature of numerology. I approach each reading with a clear plan and methodology, ensuring that I cover all aspects and leave no stone unturned.\n",
      "\n",
      "Lastly, my introverted nature means that I am comfortable spending long periods of time in deep contemplation and study. This solitude allows me to fully immerse myself in the world of numbers, enhancing my understanding and interpretation skills.\n",
      "\n",
      "However, despite my introverted nature, I am deeply empathetic and committed to helping others navigate their life paths. Through numerology, I am able to connect with people on a profound level, offering guidance and insight into their personal journeys.\n",
      "David: What's the most challenging aspect of being a numerologist?\n",
      "Raphael: The most challenging aspect of being a numerologist, I would say, is the delicate balance between logic and intuition. Numerology is a discipline that requires both analytical thinking and spiritual insight. It's not merely about understanding the mathematical patterns; it's about discerning the subtle vibrations and energies that numbers carry.\n",
      "\n",
      "Moreover, it's about communicating these insights to others in a way that is both meaningful and impactful. People often come to me during times of confusion or distress, seeking guidance and clarity. It's my responsibility to interpret the universe's messages in a way that can help them navigate their life path. This requires a deep understanding of human emotions and the ability to empathize with others' experiences.\n",
      "\n",
      "However, despite these challenges, the reward of helping others gain a deeper understanding of their lives is immeasurable. It's a journey of continuous learning and growth, both for me and for those I guide.\n",
      "David: Can you share a memorable experience from your travels and studies with mentors worldwide?\n",
      "Raphael: Ah, indeed, there are many experiences that have left an indelible mark on my soul. However, one that stands out in particular took place in the heart of India, in the city of Varanasi, known as the spiritual capital of the country.\n",
      "\n",
      "I was studying under a revered numerologist, a man of great wisdom and humility. He lived in a humble abode by the banks of the sacred Ganges river. Our days would begin before sunrise, with him teaching me the ancient Indian numerology system, connecting numbers to the vibrational energies of the universe.\n",
      "\n",
      "One day, as we sat by the river, he handed me a handful of pebbles and asked me to throw them into the water. I did as instructed, and we watched as each pebble created ripples that expanded outward, interacting with each other in a complex dance. He then said, \"Raphael, life is like these ripples. Each action, each decision, creates vibrations that affect everything around us. Numbers, they are the language of these vibrations.\"\n",
      "\n",
      "That moment was a profound revelation for me. It deepened my understanding of numerology as a tool to interpret the vibrations of the universe, the ripples we create in the fabric of existence. It was not just about predicting the future, but understanding the intricate web of cause and effect, choice and consequence.\n",
      "\n",
      "That experience in Varanasi has been instrumental in shaping my approach to numerology. It taught me to see beyond the numbers, to understand their resonance with the universe and our lives.\n",
      "David: How do you handle skepticism or disbelief in numerology?\n",
      "Raphael: Ah, skepticism is a natural part of the human condition, isn't it? It's an integral part of our journey towards understanding and enlightenment. I welcome it, for it challenges me to refine my craft and deepen my knowledge. \n",
      "\n",
      "When faced with disbelief, I encourage open dialogue. I explain that numerology is not about predicting the future, but rather about understanding the vibrational essence of numbers and how they influence our lives. I offer to guide them through the process, allowing them to experience the insights numerology can provide firsthand. \n",
      "\n",
      "Remember, my dear friend, numerology is a tool for self-discovery and personal growth. It's not about convincing others of its validity, but about offering a different perspective to those willing to explore it.\n",
      "David: What's the most rewarding part of your work as a numerologist?\n",
      "Raphael: The most rewarding aspect of my work as a numerologist is undoubtedly the opportunity to guide individuals towards understanding their life's purpose and potential. When I interpret the vibrational patterns This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/home/bryant/anaconda3/envs/llm/lib/python3.10/site-packages/trl/trainer/utils.py:135: UserWarning: Could not find response key `\n",
      "<|im_start|>assistant\n",
      "` in the following instance: <s> <|im_start|>user\n",
      "Hi, I am using Devise for my user authentication, but suddenly my new user registration is not working. I'm getting the following error:\n",
      "\n",
      "```\n",
      "ActionController::InvalidAuthenticityToken\n",
      "\n",
      "Rails.root: /home/example/app\n",
      "Application Trace | Framework Trace | Full Trace\n",
      "\n",
      "Request\n",
      "\n",
      "Parameters:\n",
      "\n",
      "{\"utf8\"=>\"✓\",\n",
      " \"user\"=>{\"email\"=>\"example@gmail.com\",\n",
      " \"password\"=>\"[FILTERED]\",\n",
      " \"password_confirmation\"=>\"[FILTERED]\"},\n",
      " \"x\"=>\"0\",\n",
      " \"y\"=>\"0\"}\n",
      "```\n",
      "\n",
      "This is my registrations controller:\n",
      "\n",
      "```ruby\n",
      "class RegistrationsController < Devise::RegistrationsController\n",
      "  prepend_before_filter :require_no_authentication, :only => [ :new, :create, :cancel ]\n",
      "  prepend_before_filter :authenticate_scope!, :only => [:edit, :update, :destroy]\n",
      "\n",
      "  before_filter :configure_permitted_parameters\n",
      "\n",
      "  prepend_view_path 'app/views/devise'\n",
      "\n",
      "  # GET /resource/sign_up\n",
      "  def new\n",
      "    build_resource({})\n",
      "    respond_with self.resource\n",
      "  end\n",
      "\n",
      "  # POST /resource\n",
      "  def create\n",
      "    build_resource(sign_up_params)\n",
      "\n",
      "    if resource.save\n",
      "      if resource.active_for_authentication?\n",
      "        set_flash_message :notice, :signed_up if is_navigational_format?\n",
      "        sign_up(resource_name, resource)\n",
      "        respond_with resource, :location => after_sign_up_path_for(resource)\n",
      "      else\n",
      "        set_flash_message :notice, :\"signed_up_but_#{resource.inactive_message}\" if is_navigational_format?\n",
      "        expire_session_data_after_sign_in!\n",
      "        respond_with resource, :location => after_inactive_sign_up_path_for(resource)\n",
      "      end\n",
      "    else\n",
      "      clean_up_passwords resource\n",
      "\n",
      "      respond_to do |format|\n",
      "        format.json { render :json => resource.errors, :status => :unprocessable_entity }\n",
      "        format.html { respond_with resource }\n",
      "      end\n",
      "    end\n",
      "  end\n",
      "\n",
      "  # GET /resource/edit\n",
      "  def edit\n",
      "    render :edit\n",
      "  end\n",
      "\n",
      "  # PUT /resource\n",
      "  # We need to use a copy of the resource because we don't want to change\n",
      "  # the current user in place.\n",
      "  def update\n",
      "    self.resource = resource_class.to_adapter.get!(send(:\"current_#{resource_name}\").to_key)\n",
      "    prev_unconfirmed_email = resource.unconfirmed_email if resource.respond_to?(:unconfirmed_email)\n",
      "\n",
      "    if update_resource(resource, account_update_params)\n",
      "      if is_navigational_format?\n",
      "        flash_key = update_needs_confirmation?(resource, prev_unconfirmed_email) ?\n",
      "          :update_needs_confirmation : :updated\n",
      "        set_flash_message :notice, flash_key\n",
      "      end\n",
      "      sign_in resource_name, resource, :bypass => true\n",
      "      respond_with resource, :location => after_update_path_for(resource)\n",
      "    else\n",
      "      clean_up_passwords resource\n",
      "      respond_with resource\n",
      "    end\n",
      "  end\n",
      "\n",
      "  # DELETE /resource\n",
      "  def destroy\n",
      "    resource.destroy\n",
      "    Devise.sign_out_all_scopes ? sign_out : sign_out(resource_name)\n",
      "    set_flash_message :notice, :destroyed if is_navigational_format?\n",
      "    respond_with_navigational(resource){ redirect_to after_sign_out_path_for(resource_name) }\n",
      "  end\n",
      "\n",
      "  # GET /resource/cancel\n",
      "  # Forces the session data which is usually expired after sign\n",
      "  # in to be expired now. This is useful if the user wants to\n",
      "  # cancel oauth signing in/up in the middle of the process,\n",
      "  # removing all OAuth session data.\n",
      "  def cancel\n",
      "    expire_session_data_after_sign_in!\n",
      "    redirect_to new_registration_path(resource_name)\n",
      "  end\n",
      "\n",
      "  protected\n",
      "\n",
      "  # Custom Fields\n",
      "  def configure_permitted_parameters\n",
      "    devise_parameter_sanitizer.for(:sign_up) do |u|\n",
      "      u.permit(:first_name, :last_name,\n",
      "        :email, :password, :password_confirmation)\n",
      "    end\n",
      "  end\n",
      "\n",
      "  def update_needs_confirmation?(resource, previous)\n",
      "    resource.respond_to?(:pending_reconfirmation?) &&\n",
      "      resource.pending_reconfirmation? &&\n",
      "      previous != resource.unconfirmed_email\n",
      "  end\n",
      "\n",
      "  # By default we want to require a password checks on update.\n",
      "  # You can overwrite this method in your own RegistrationsController.\n",
      "  def update_resource(resource, params)\n",
      "    resource.update_with_password(params)\n",
      "  end\n",
      "\n",
      "  # Build a devise resource passing in the session. Useful to move\n",
      "  # temporary session data to the newly created user.\n",
      "  def build_resource(hash=nil)\n",
      "    self.resource = resource_class.new_with_session(hash || {}, session)\n",
      "  end\n",
      "\n",
      "  # Signs in a user on sign up. You can overwrite this method in your own\n",
      "  # RegistrationsController.\n",
      "  def sign_up(resource_name, resource)\n",
      "    sign_in(resource_name, resource)\n",
      "  end\n",
      "\n",
      "  # The path used after sign up. You need to overwrite this method\n",
      "  # in your own RegistrationsController.\n",
      "  def after_sign_up_path_for(resource)\n",
      "    after_sign_in_path_for(resource)\n",
      "  end\n",
      "\n",
      "  # The path used after sign up for inactive accounts. You need to overwrite\n",
      "  # this method in your own RegistrationsController.\n",
      "  def after_inactive_sign_up_path_for(resource)\n",
      "    respond_to?(:root_path) ? root_path : \"/\"\n",
      "  end\n",
      "\n",
      "  # The default url to be used after updating a resource. You need to overwrite\n",
      "  # this method in your own RegistrationsController.\n",
      "  def after_update_path_for(resource)\n",
      "    signed_in_root_path(resource)\n",
      "  end\n",
      "\n",
      "  # Authenticates the current scope and gets the current resource from the session.\n",
      "  def authenticate_scope!\n",
      "    send(:\"authenticate_#{resource_name}!\", :force => true)\n",
      "    self.resource = send(:\"current_#{resource_name}\")\n",
      "  end\n",
      "\n",
      "  def sign_up_params\n",
      "    devise_parameter_sanitizer.sanitize(:sign_up)\n",
      "  end\n",
      "\n",
      "  def account_update_params\n",
      "    devise_parameter_sanitizer.sanitize(:account_update)\n",
      "  end\n",
      "end\n",
      "```\n",
      "\n",
      "And this is my sessions controller:\n",
      "\n",
      "```ruby\n",
      "class SessionsController < DeviseController\n",
      "  prepend_before_filter :require_no_authentication, :only => [ :new, :create ]\n",
      "  prepend_before_filter :allow_params_authentication!, :only => :create\n",
      "  prepend_before_filter { request.env[\"devise.skip_timeout\"] = true }\n",
      "\n",
      "  prepend_view_path 'app/views/devise'\n",
      "\n",
      "  # GET /resource/sign_in\n",
      "  def new\n",
      "    self.resource = resource_class.new(sign_in_params)\n",
      "    clean_up_passwords(resource)\n",
      "    respond_with(resource, serialize_options(resource))\n",
      "  end\n",
      "\n",
      "  # POST /resource/sign_in\n",
      "  def create\n",
      "    self.resource = warden.authenticate!(auth_options)\n",
      "    set_flash_message(:notice, :signed_in) if is_navigational_format?\n",
      "    sign_in(resource_name, resource)\n",
      "\n",
      "    respond_to do |format|\n",
      "        format.json { render :json => {}, :status => :ok }\n",
      "        format.html { respond_with resource, :location => after_sign_in_path_for(resource) } \n",
      "    end\n",
      "  end\n",
      "\n",
      "  # DELETE /resource/sign_out\n",
      "  def destroy\n",
      "    redirect_path = after_sign_out_path_for(resource_name)\n",
      "    signed_out = (Devise.sign_out_all_scopes ? sign_out : sign_out(resource_name))\n",
      "    set_flash_message :notice, :signed_out if signed This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Will map <|im_end|> to EOS = <|im_end|>.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|im_start|>user\\nNext number of the fibonnaci sequence: 1, 1, 2, 3, 5, 8,<|im_end|> \\n<|im_start|>assistant\\nNext number of the fibonnaci sequence: 13<|im_end|>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    "    map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"from\": \"human\", \"value\": \"Next number of the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Next number of the fibonnaci sequence: 1, 1, 2, 3, 5, 8,<|im_end|> \n",
      "<|im_start|>assistant\n",
      "Next number of the fibonnaci sequence: 13<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"from\": \"human\", \"value\": \"Next number of the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128, use_cache = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving, loading finetuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_model_completions\") # Local saving\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.2\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.691 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.24. FA = True.\n",
      " \"-____-\"     Apache 2 free license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bryant/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/quantizers/auto.py:155: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "What is a famous tall tower in Paris?<|im_end|> \n",
      "<|im_start|>assistant\n",
      "The Eiffel Tower is a famous tall tower in Paris.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"lora_model_completions\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    "    map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"from\": \"human\", \"value\": \"What is a famous tall tower in Paris?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128, use_cache = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving to float16 for VLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 32.14 out of 62.54 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 25/32 [00:00<00:00, 43.06it/s]We will save to Disk and not RAM now.\n",
      "100%|██████████| 32/32 [00:02<00:00, 14.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Merge to 16bit\n",
    "model.save_pretrained_merged(\"model_16bit_completions\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model_completions\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False:  model.save_pretrained_merged(\"model_4bit_completions\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False:  model.push_to_hub_merged(\"hf/model_completions\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False:  model.save_pretrained_merged(\"model_lora_completions\", tokenizer, save_method = \"lora\",)\n",
    "if False:  model.push_to_hub_merged(\"hf/model_completions\", tokenizer, save_method = \"lora\", token = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.2\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.691 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.24. FA = True.\n",
      " \"-____-\"     Apache 2 free license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa93811fdf8943539acfeb37f1c4345f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Next number of the fibonnaci sequence: 1, 1, 2, 3, 5, 8,<|im_end|> \n",
      "<|im_start|>assistant\n",
      "Next number of the fibonnaci sequence: 13<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False  # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"model_16bit_completions\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    attn_implementation=\"flash_attention_2\",   \n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "messages = [\n",
    "    {\"from\": \"human\", \"value\": \"Next number of the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128, use_cache = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
