{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define input prompts & SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.2, top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_start|>user\\nNext number of the fibonnaci sequence: 1, 1, 2, 3, 5, 8,<|im_end|>\\n<|im_start|>assistant\\n',\n",
       " '<|im_start|>user\\n台灣最高的建築物是什麼？<|im_end|>\\n<|im_start|>assistant\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"model_16bit\")\n",
    "\n",
    "messages_list = [\n",
    "    [\n",
    "        {\"from\": \"human\", \"value\": \"Next number of the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "    ],\n",
    "    [\n",
    "        {\"from\": \"human\", \"value\": \"台灣最高的建築物是什麼？\"},\n",
    "    ],\n",
    "]\n",
    "\n",
    "prompts = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = False,\n",
    "        add_generation_prompt = True,\n",
    "    ) for messages in messages_list\n",
    "]\n",
    "prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init vLLM engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-04 14:07:37 llm_engine.py:79] Initializing an LLM engine with config: model='model_16bit', tokenizer='model_16bit', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2000, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "INFO 03-04 14:08:05 llm_engine.py:337] # GPU blocks: 662, # CPU blocks: 2048\n",
      "INFO 03-04 14:08:06 model_runner.py:676] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-04 14:08:06 model_runner.py:680] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 03-04 14:08:09 model_runner.py:748] Graph capturing finished in 3 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"model_16bit\", gpu_memory_utilization=0.65, max_model_len=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:00<00:00,  4.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: '<|im_start|>user\\nNext number of the fibonnaci sequence: 1, 1, 2, 3, 5, 8,<|im_end|>\\n<|im_start|>assistant\\n', Generated text: 'The next number in the Fibonacci sequence is 13.'\n",
      "Prompt: '<|im_start|>user\\n台灣最高的建築物是什麼？<|im_end|>\\n<|im_start|>assistant\\n', Generated text: '台灣最高的建築物是台北101'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=0, prompt='<|im_start|>user\\nNext number of the fibonnaci sequence: 1, 1, 2, 3, 5, 8,<|im_end|>\\n<|im_start|>assistant\\n', prompt_token_ids=[1, 523, 28766, 321, 28730, 2521, 28766, 28767, 1838, 13, 6693, 1474, 302, 272, 16182, 266, 28711, 13114, 7768, 28747, 28705, 28740, 28725, 28705, 28740, 28725, 28705, 28750, 28725, 28705, 28770, 28725, 28705, 28782, 28725, 28705, 28783, 28725, 2, 28705, 13, 28789, 28766, 321, 28730, 2521, 28766, 28767, 489, 11143, 13], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='The next number in the Fibonacci sequence is 13.', token_ids=[1014, 1679, 1474, 297, 272, 401, 593, 266, 28127, 7768, 349, 28705, 28740, 28770, 28723, 2], cumulative_logprob=0.0, logprobs=None, finish_reason=length)], finished=True, metrics=RequestMetrics(arrival_time=347032.245817534, last_token_time=347032.245817534, first_scheduled_time=1709532497.8421493, first_token_time=1709532498.0150914, time_in_queue=1709185465.5963318, finished_time=1709532498.3162298), lora_request=None),\n",
       " RequestOutput(request_id=1, prompt='<|im_start|>user\\n台灣最高的建築物是什麼？<|im_end|>\\n<|im_start|>assistant\\n', prompt_token_ids=[1, 523, 28766, 321, 28730, 2521, 28766, 28767, 1838, 13, 29677, 31521, 29190, 29366, 28914, 29193, 234, 178, 140, 29535, 28971, 30289, 236, 189, 191, 29771, 2, 28705, 13, 28789, 28766, 321, 28730, 2521, 28766, 28767, 489, 11143, 13], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='台灣最高的建築物是台北101', token_ids=[29677, 31521, 29190, 29366, 28914, 29193, 234, 178, 140, 29535, 28971, 29677, 29799, 28740, 28734, 28740], cumulative_logprob=-0.6931471824645996, logprobs=None, finish_reason=length)], finished=True, metrics=RequestMetrics(arrival_time=347032.263878811, last_token_time=347032.263878811, first_scheduled_time=1709532497.8421493, first_token_time=1709532498.0150914, time_in_queue=1709185465.5782704, finished_time=1709532498.316235), lora_request=None)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Completions API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion result: Completion(id='cmpl-d23138fc33de4d9a8632e6203e28e8b9', choices=[CompletionChoice(finish_reason='length', index=0, logprobs=None, text='台北101，高度達到508公尺，')], created=347455, model='model_16bit', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=16, prompt_tokens=13, total_tokens=29))\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:7788/v1\"\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "completion = client.completions.create(model=\"model_16bit\",\n",
    "                                      prompt=\"台灣最高的建築物是\",\n",
    "                                      temperature=0.1) # max_tokens 和 temperature 參數在此設定\n",
    "                                      \n",
    "print(\"Completion result:\", completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Chat API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat response: ChatCompletion(id='cmpl-9e636679178c4d0f981611b05864ab6f', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Q: There's a theory which states that if ever anyone discovers exactly what the Universe is for and why it is here, it will instantly disappear and be replaced by something even more bizarre and inexplicable. There's another theory which states that this has already happened.\", role='assistant', function_call=None, tool_calls=None))], created=347176, model='model_16bit', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=60, prompt_tokens=46, total_tokens=106))\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "# Set OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:7788/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"model_16bit\",\n",
    "    messages=[\n",
    "        {\"from\": \"system\", \"value\": \"You are a helpful assistant.\"},\n",
    "        {\"from\": \"human\", \"value\": \"Tell me a joke.\"},\n",
    "    ]\n",
    ")\n",
    "print(\"Chat response:\", chat_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
